{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61ebd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "#word embedding\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "#models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3efa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('training_set.txt', sep='\\t')\n",
    "dev_set = pd.read_csv('dev_set.txt', sep='\\t')\n",
    "test_set = pd.read_csv('test_set.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911f42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "exclude.remove(\"!\")\n",
    "exclude.remove(\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5204371",
   "metadata": {},
   "source": [
    "# Don't removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc40629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list, lemmatize, stemmer):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    \n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        text = text.lower()\n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        text = re.sub(\"[0-9]\", ' ', text)\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        #text = \" \".join([word for word in text.split() if word not in stop ])\n",
    "        \n",
    "        text = \" \".join([word for word in text.split() if word not in exclude ])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(snowball_stemmer.stem(word) for word in text.split())\n",
    "            \n",
    "       \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ab1b5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb5f6dad19c438fa5de9e1c0e418c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9de3d025f67415897fb37b463043be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_set['sentence'] = clean(train_set['sentence'], lemmatize = True, stemmer = False)\n",
    "dev_set['sentence'] = clean(dev_set['sentence'], lemmatize = True, stemmer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1020507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.61      0.35      0.44       369\n",
      "Anticipation       0.51      0.44      0.48       196\n",
      "     Disgust       0.09      0.22      0.13        32\n",
      "        Fear       0.24      0.37      0.29        68\n",
      "         Joy       0.38      0.45      0.41        82\n",
      "     Sadness       0.29      0.37      0.32        67\n",
      "    Surprise       0.18      0.39      0.24        44\n",
      "       Trust       0.36      0.40      0.38       142\n",
      "\n",
      "    accuracy                           0.38      1000\n",
      "   macro avg       0.33      0.37      0.34      1000\n",
      "weighted avg       0.45      0.38      0.40      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(max_features=14000,ngram_range=(1,2))\n",
    "X = tfid.fit_transform(train_set['sentence'])\n",
    "y = np.array(train_set[\"emotion\"])\n",
    "Test = tfid.transform(dev_set['sentence'])\n",
    "y_hat = np.array(dev_set[\"emotion\"])\n",
    "model = SVC(C=0.85,kernel='linear')\n",
    "model.fit(X,y)\n",
    "predict = model.predict(Test)\n",
    "labels = {\"Anger\": 1, \"Anticipation\": 2, \"Disgust\": 3, \"Fear\": 4, \"Joy\": 5, \"Sadness\": 6, \"Surprise\": 7, \"Trust\": 8}\n",
    "print (classification_report(predict, y_hat, target_names=labels.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb8426",
   "metadata": {},
   "source": [
    "# Using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b135e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list, lemmatize, stemmer):\n",
    "    \"\"\"\n",
    "    Function that a receives a list of strings and preprocesses it.\n",
    "    \n",
    "    :param text_list: List of strings.\n",
    "    :param lemmatize: Tag to apply lemmatization if True.\n",
    "    :param stemmer: Tag to apply the stemmer if True.\n",
    "    \"\"\"\n",
    "    \n",
    "    updates = []\n",
    "    for j in tqdm(range(len(text_list))):\n",
    "        \n",
    "        text = text_list[j]\n",
    "        text = text.lower()\n",
    "        #REMOVE THAT IS NOT TEXT\n",
    "        text = re.sub(\"[0-9]\", ' ', text)\n",
    "        text = re.sub(\"'\", ' ', text, flags=re.I)\n",
    "        text = re.sub('(\\[|\\])',' ',text)\n",
    "        \n",
    "        text = re.sub(':','',text) \n",
    "        text = re.sub(';','',text) \n",
    "        text = re.sub('--','',text) \n",
    "        text = re.sub('soltáis','',text)\n",
    "        text = re.sub('empathicalist','',text)\n",
    "        text = re.sub('\\*','',text)\n",
    "        text = re.sub('-',' - ',text)\n",
    "        text = re.sub('nuggies','',text)\n",
    "        text = re.sub('sourcastic','',text)\n",
    "        text = re.sub('buljanoff','person',text)\n",
    "        text = re.sub('verua','',text)\n",
    "        text = re.sub('amalgate','combine',text)\n",
    "        text = re.sub('approvechamos','',text) \n",
    "        text = re.sub('dogl','dog',text) \n",
    "        text = re.sub('helaros','ice cream',text)\n",
    "        text = re.sub('ruuuuuth !','ruth !!!!!',text)\n",
    "        text = re.sub('radarjockeys','',text)\n",
    "        text = re.sub('favrinis','favrini',text)\n",
    "        text = re.sub('tadminster','',text)\n",
    "        text = re.sub('buckoes','bucko',text)\n",
    "        text = re.sub('päätti','',text)\n",
    "        text = re.sub('teidät','',text)\n",
    "        text = re.sub('tilalle','',text)\n",
    "        text = re.sub('volavent','pie',text)\n",
    "        text = re.sub('päiväsi on lopussa','your day is over',text)\n",
    "        text = re.sub('obsolescing','obsolesce',text)\n",
    "        text = re.sub('adiran','',text)\n",
    "        text = re.sub('shikseh','food',text)\n",
    "        text = re.sub('hundjager','animal',text)\n",
    "        text = re.sub('devriess','person',text)\n",
    "        text = re.sub('shalakazam','magic',text)\n",
    "        text = re.sub('=','equal',text)\n",
    "        text = re.sub('dovitch ','person',text)\n",
    "        text = re.sub('chewgood ','',text)\n",
    "        text = re.sub('kamoja ','',text)\n",
    "        text = re.sub('hourses ','horses',text)\n",
    "        text = re.sub('disppearance','disappearance',text)\n",
    "        text = re.sub('monosyllabically','',text)\n",
    "        text = re.sub('alonger','',text)\n",
    "        text = re.sub('oextry','',text)\n",
    "        text = re.sub('azupep', '',text)\n",
    "        text = re.sub('suitcoat', '',text)\n",
    "        text = re.sub('scuzzball', 'disgusting person',text)\n",
    "        text = re.sub('bited', 'bit',text)\n",
    "        text = re.sub('antinuke', 'anti nuke',text)\n",
    "        text = re.sub('jouncing', 'jounce',text)\n",
    "        text = re.sub('neverjoke', 'never joke',text)\n",
    "        text = re.sub('couid', 'could',text)\n",
    "        text = re.sub('lmprovise', 'improvise',text)\n",
    "        text = re.sub('pecmans', '',text)\n",
    "        text = re.sub('buttissimo', 'book',text)\n",
    "        text = re.sub('rakonin', '',text)\n",
    "        \n",
    "        aux = []\n",
    "        for i in text.split():\n",
    "            i = re.sub(r'[\\w\\s]+[?.!:]+$', i.translate(str.maketrans('', '', string.punctuation)), i)\n",
    "            aux.append(re.sub(r'[.!?:]+', ' '.join(i), i))\n",
    "\n",
    "        text = ' '.join(aux)\n",
    "\n",
    "        \n",
    "        #REMOVE STOP WORDS\n",
    "        #text = \" \".join([word for word in text.split() if word not in stop ])\n",
    "        \n",
    "        #text = \" \".join([word for word in text.split() if word not in exclude ])\n",
    "        \n",
    "        #LEMMATIZATION\n",
    "        if lemmatize:\n",
    "            text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #STEMMER\n",
    "        if stemmer:\n",
    "            text = \" \".join(snowball_stemmer.stem(word) for word in text.split())\n",
    "            \n",
    "        text = re.sub('breechclouted','breechclout',text) \n",
    "        text = re.sub('coldcocked','coldcock',text) \n",
    "        text = re.sub('upchucked','upchuck',text) \n",
    "        \n",
    "        \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "266998aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243de06e8885450d86b699891b87c3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed40c94548d4d77b751560ea5ba2a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1b68c42451433489a97a368048b32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = pd.read_csv('training_set.txt', sep='\\t')\n",
    "dev_set = pd.read_csv('dev_set.txt', sep='\\t')\n",
    "test_set = pd.read_csv('test_set.txt', sep='\\t')\n",
    "\n",
    "train_set['sentence'] = clean(train_set['sentence'], lemmatize = False, stemmer = False)\n",
    "dev_set['sentence'] = clean(dev_set['sentence'], lemmatize = False, stemmer = False)\n",
    "test_set['sentence'] = clean(test_set['sentence'], lemmatize = False, stemmer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86efeba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.en.vec: 6.60GB [05:18, 20.7MB/s]                                                                    \n",
      "  0%|                                                                                      | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████████████████████████████████████████████████████████████████| 2519370/2519370 [08:03<00:00, 5208.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2519370 words\n"
     ]
    }
   ],
   "source": [
    "#downloading the vocab from the FastText\n",
    "fasttext = vocab.FastText()\n",
    "print('Loaded {} words'.format(len(fasttext.itos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15e51404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sentenc_to_vector(df):\n",
    "    '''\n",
    "    Function to transform sentence to a vector of numbers using the FastText embedding \n",
    "    df: data frame with a column namede 'sentence' that will be transformed.\n",
    "    To transform the sentence into a vector of numbers we took the mean\n",
    "    '''\n",
    "    emb = []\n",
    "    for i in range(len(df)):\n",
    "        aux=[]\n",
    "        words = df['sentence'][i].split()\n",
    "        for k in words:\n",
    "            aux.append(np.array(fasttext.vectors[fasttext.stoi[k]]))\n",
    "        emb.append(pd.Series(np.mean(aux,axis=0)))\n",
    "\n",
    "    return pd.DataFrame(emb)\n",
    "\n",
    "emb_df_train = sentenc_to_vector(train_set)\n",
    "emb_df_dev = sentenc_to_vector(dev_set)\n",
    "emb_df_test = sentenc_to_vector(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd25d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "log_reg_params = [{\"C\":0.01},\n",
    "                  {\"C\":0.1},\n",
    "                  {\"C\":1},\n",
    "                  {\"C\":1, \"max_iter\":10000},\n",
    "                  {\"C\":1, \"solver\":'newton-cg', \"max_iter\":10000},\n",
    "                  {\"C\":1, \"solver\":'liblinear'},\n",
    "                  {\"C\":10, \"solver\":'liblinear'},\n",
    "                  {\"C\":10, \"solver\":'newton-cg'},\n",
    "                  {\"C\":10, \"max_iter\":10000},\n",
    "                  {\"C\":100, \"max_iter\":10000}]\n",
    "\n",
    "dec_tree_params = [{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}]\n",
    "\n",
    "rand_for_params = [{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"},\n",
    "                   {\"criterion\": \"gini\",\"n_estimators\":1000, \"max_depth\":50}\n",
    "                  ]\n",
    "\n",
    "kneighbors_params = [{\"n_neighbors\":3}, {\"n_neighbors\":5}, {\"n_neighbors\":10}]\n",
    "\n",
    "naive_bayes_params = [{}]\n",
    "svc_params = [{\"C\" :0.10, \"kernel\":'rbf' ,\"degree\": 3, \"gamma\":'scale'}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}  ]\n",
    "\n",
    "modelclasses = [\n",
    "    [\"log regression\", LogisticRegression, log_reg_params],\n",
    "    [\"decision tree\", DecisionTreeClassifier, dec_tree_params],\n",
    "    [\"random forest\", RandomForestClassifier, rand_for_params],\n",
    "    [\"k neighbors\", KNeighborsClassifier, kneighbors_params],\n",
    "    [\"naive bayes\", GaussianNB, naive_bayes_params],\n",
    "    [\"support vector machines\", SVC, svc_params]\n",
    "]\n",
    "\n",
    "x_train = np.array(emb_df_train)\n",
    "y_train = np.array(train_set[\"emotion\"])\n",
    "x_test = np.array(emb_df_dev)\n",
    "y_test = np.array(dev_set[\"emotion\"])\n",
    "\n",
    "\n",
    "insights = []\n",
    "for modelname, Model, params_list in modelclasses:\n",
    "    for params in params_list:\n",
    "        model = Model(**params)\n",
    "        model.fit(x_train, y_train)\n",
    "        score = model.score(x_test, y_test)\n",
    "        insights.append((modelname, model, params, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ed7c0f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support vector machines {'C': 10} 0.429\n",
      "support vector machines {'C': 1} 0.428\n",
      "log regression {'C': 1} 0.421\n",
      "log regression {'C': 1, 'max_iter': 10000} 0.421\n",
      "log regression {'C': 1, 'solver': 'newton-cg', 'max_iter': 10000} 0.42\n",
      "log regression {'C': 1, 'solver': 'liblinear'} 0.419\n",
      "log regression {'C': 10, 'solver': 'liblinear'} 0.41\n",
      "log regression {'C': 10, 'solver': 'newton-cg'} 0.41\n",
      "log regression {'C': 10, 'max_iter': 10000} 0.41\n",
      "log regression {'C': 100, 'max_iter': 10000} 0.41\n",
      "log regression {'C': 0.1} 0.401\n",
      "random forest {'criterion': 'gini', 'n_estimators': 1000, 'max_depth': 50} 0.359\n",
      "support vector machines {'C': 0.1, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale'} 0.336\n",
      "support vector machines {'C': 0.1} 0.336\n",
      "random forest {'criterion': 'gini'} 0.335\n",
      "random forest {'criterion': 'entropy'} 0.331\n",
      "k neighbors {'n_neighbors': 10} 0.307\n",
      "log regression {'C': 0.01} 0.298\n",
      "k neighbors {'n_neighbors': 5} 0.287\n",
      "naive bayes {} 0.28\n",
      "k neighbors {'n_neighbors': 3} 0.262\n",
      "decision tree {'criterion': 'entropy'} 0.191\n",
      "decision tree {'criterion': 'gini'} 0.19\n"
     ]
    }
   ],
   "source": [
    "insights.sort(key=lambda x:x[-1], reverse=True)\n",
    "for modelname, model, params, score in insights:\n",
    "    print(modelname, params, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335dc5f",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b78a537",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.65      0.40      0.49       346\n",
      "Anticipation       0.49      0.44      0.46       187\n",
      "     Disgust       0.22      0.52      0.31        33\n",
      "        Fear       0.23      0.46      0.31        52\n",
      "         Joy       0.45      0.47      0.46        94\n",
      "     Sadness       0.37      0.41      0.39        79\n",
      "    Surprise       0.24      0.41      0.30        56\n",
      "       Trust       0.44      0.46      0.45       153\n",
      "\n",
      "    accuracy                           0.43      1000\n",
      "   macro avg       0.39      0.44      0.40      1000\n",
      "weighted avg       0.49      0.43      0.44      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C= 0.9)\n",
    "model.fit(x_train,y_train)\n",
    "predict = model.predict(x_test)\n",
    "labels = {\"Anger\": 1, \"Anticipation\": 2, \"Disgust\": 3, \"Fear\": 4, \"Joy\": 5, \"Sadness\": 6, \"Surprise\": 7, \"Trust\": 8}\n",
    "print (classification_report(predict, y_test, target_names=labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82bb4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.71      0.42      0.53      5049\n",
      "Anticipation       0.55      0.48      0.51      2440\n",
      "     Disgust       0.26      0.60      0.36       581\n",
      "        Fear       0.25      0.61      0.35       576\n",
      "         Joy       0.54      0.65      0.59      1233\n",
      "     Sadness       0.41      0.49      0.45      1156\n",
      "    Surprise       0.32      0.52      0.40       708\n",
      "       Trust       0.48      0.45      0.46      2257\n",
      "\n",
      "    accuracy                           0.48     14000\n",
      "   macro avg       0.44      0.53      0.46     14000\n",
      "weighted avg       0.55      0.48      0.49     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(model.predict(x_train), y_train, target_names=labels.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a1b17",
   "metadata": {},
   "source": [
    "# Exporting the results form Dev set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85b4ebc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_new=pd.concat([dev_set['sentence'],pd.Series(model.predict(x_test))], axis=1).rename(columns={0:'emotion_pred'})\n",
    "dev_new.to_csv('dev_set_prep.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "564ead1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(emb_df_test)\n",
    "\n",
    "test_new=pd.concat([test_set['sentence'],pd.Series(model.predict(test))], axis=1).rename(columns={0:'emotion_pred'})\n",
    "test_new.to_csv('test_set_prep.txt',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
